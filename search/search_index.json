{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_2","title":"Home","text":""},{"location":"#_3","title":"Home","text":"<p>AutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&amp;M University. The goal of AutoKeras is to make machine learning accessible to everyone.</p>"},{"location":"#learning-resources","title":"Learning resources","text":"<ul> <li>A short example.</li> </ul> <pre><code>import autokeras as ak\n\nclf = ak.ImageClassifier()\nclf.fit(x_train, y_train)\nresults = clf.predict(x_test)\n</code></pre> <ul> <li>Official website tutorials.</li> <li>The book of Automated Machine Learning in Action.</li> <li>The LiveProjects of Image Classification with AutoKeras.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the package, please use the <code>pip</code> installation as follows:</p> <pre><code>pip3 install autokeras\n</code></pre> <p>Please follow the installation guide for more details.</p> <p>Note: Currently, AutoKeras is only compatible with Python &gt;= 3.7 and TensorFlow &gt;= 2.8.0.</p>"},{"location":"#community","title":"Community","text":"<p>Ask your questions on our GitHub Discussions.</p>"},{"location":"#contributing-code","title":"Contributing Code","text":"<p>Here is how we manage our project.</p> <p>We pick the critical issues to work on from GitHub issues. They will be added to this Project. Some of the issues will then be added to the milestones, which are used to plan for the releases.</p> <p>Refer to our Contributing Guide to learn the best practices.</p> <p>Thank all the contributors!</p> <p></p>"},{"location":"#cite-this-work","title":"Cite this work","text":"<p>Haifeng Jin, Fran\u00e7ois Chollet, Qingquan Song, and Xia Hu. \"AutoKeras: An AutoML Library for Deep Learning.\" the Journal of machine Learning research 6 (2023): 1-6. (Download)</p> <p>Biblatex entry:</p> <pre><code>@article{JMLR:v24:20-1355,\n  author  = {Haifeng Jin and Fran\u00e7ois Chollet and Qingquan Song and Xia Hu},\n  title   = {AutoKeras: An AutoML Library for Deep Learning},\n  journal = {Journal of Machine Learning Research},\n  year    = {2023},\n  volume  = {24},\n  number  = {6},\n  pages   = {1--6},\n  url     = {http://jmlr.org/papers/v24/20-1355.html}\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&amp;M College of Engineering, and Texas A&amp;M University.</p>"},{"location":"about/","title":"About","text":"<p>This package is developed by DATA LAB at Texas A&amp;M University, collaborating with keras-team for version 1.0 and above.</p>"},{"location":"about/#core-team","title":"Core Team","text":"<p>Haifeng Jin: Created, designed and implemented the AutoKeras system.  Maintainer.</p> <p>Fran\u00e7ois Chollet: The API and system architecture design for AutoKeras 1.0. Code reviews for pull requests.</p> <p>Qingquan Song: Designed the neural architecture search algorithms. Implemented the tabular data classification and regression module.</p> <p>Xia \"Ben\" Hu: Project lead and maintainer.</p>"},{"location":"auto_model/","title":"AutoModel","text":"<p>[source]</p>"},{"location":"auto_model/#automodel","title":"AutoModel","text":"<pre><code>autokeras.AutoModel(\n    inputs,\n    outputs,\n    project_name=\"auto_model\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=\"greedy\",\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has <code>fit()</code> and  <code>predict()</code> methods.</p> <p>The AutoModel has two use cases. In the first case, the user only specifies the input nodes and output heads of the AutoModel. The AutoModel infers the rest part of the model. In the second case, user can specify the high-level architecture of the AutoModel by connecting the Blocks with the functional API, which is the same as the Keras functional API.</p> <p>Exampl</p>"},{"location":"auto_model/#example","title":"Example","text":"<p><pre><code>    # The user only specifies the input nodes and output heads.\n    import autokeras as ak\n    ak.AutoModel(\n        inputs=[ak.ImageInput(), ak.TextInput()],\n        outputs=[ak.ClassificationHead(), ak.RegressionHead()]\n    )\n</code></pre> <pre><code>    # The user specifies the high-level architecture.\n    import autokeras as ak\n    image_input = ak.ImageInput()\n    image_output = ak.ImageBlock()(image_input)\n    text_input = ak.TextInput()\n    text_output = ak.TextBlock()(text_input)\n    output = ak.Merge()([image_output, text_output])\n    classification_output = ak.ClassificationHead()(output)\n    regression_output = ak.RegressionHead()(output)\n    ak.AutoModel(\n        inputs=[image_input, text_input],\n        outputs=[classification_output, regression_output]\n    )\n</code></pre></p> <p>Arguments</p> <ul> <li>inputs <code>autokeras.Input | List[autokeras.Input]</code>: A list of Node instances.     The input node(s) of the AutoModel.</li> <li>outputs <code>autokeras.Head | autokeras.Node | list</code>: A list of Node or Head instances.     The output node(s) or head(s) of the AutoModel.</li> <li>project_name <code>str</code>: String. The name of the AutoModel. Defaults to     'auto_model'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner]</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. Defaults to 'greedy'.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by keras_tuner.Tuner.</li> </ul> <p>[source]</p>"},{"location":"auto_model/#fit","title":"fit","text":"<pre><code>AutoModel.fit(\n    x=None,\n    y=None,\n    batch_size=32,\n    epochs=None,\n    callbacks=None,\n    validation_split=0.2,\n    validation_data=None,\n    verbose=1,\n    **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray or tensorflow.Dataset. Training data x.</li> <li>y: numpy.ndarray or tensorflow.Dataset. Training data y.</li> <li>batch_size: Int. Number of samples per gradient update. Defaults to     32.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2.     Fraction of the training data to be used as validation data.     The model will set apart this fraction of the training data,     will not train on it, and will evaluate the loss and any model     metrics on this data at the end of each epoch.  The validation     data is selected from the last samples in the <code>x</code> and <code>y</code> data     provided, before shuffling. This argument is not supported when     <code>x</code> is a dataset. The best model found would be fit on the     entire dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar,     2 = one line per epoch. Note that the progress bar is not     particularly useful when logged to a file, so verbose=2 is     recommended when not running interactively (eg, in a production     environment). Controls the verbosity of both KerasTuner search     and     keras.Model.fit</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"auto_model/#predict","title":"predict","text":"<pre><code>AutoModel.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"auto_model/#evaluate","title":"evaluate","text":"<pre><code>AutoModel.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"auto_model/#export_model","title":"export_model","text":"<pre><code>AutoModel.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"base/","title":"Base Class","text":"<p>[source]</p>"},{"location":"base/#node","title":"Node","text":"<pre><code>autokeras.Node(**kwargs)\n</code></pre> <p>The nodes in a network connecting the blocks.</p> <p>[source]</p>"},{"location":"base/#block","title":"Block","text":"<pre><code>autokeras.Block(**kwargs)\n</code></pre> <p>The base class for different Block.</p> <p>The Block can be connected together to build the search space for an AutoModel. Notably, many args in the init function are defaults to be a tunable variable when not specified by the user.</p> <p>[source]</p>"},{"location":"base/#build","title":"build","text":"<pre><code>Block.build(hp, inputs=None)\n</code></pre> <p>Build the Block into a real Keras Model.</p> <p>The subclasses should override this function and return the output node.</p> <p>Arguments</p> <ul> <li>hp: HyperParameters. The hyperparameters for building the model.</li> <li>inputs: A list of input node(s).</li> </ul> <p>[source]</p>"},{"location":"base/#head","title":"Head","text":"<pre><code>autokeras.Head(loss=None, metrics=None, **kwargs)\n</code></pre> <p>Base class for the heads, e.g. classification, regression.</p> <p>Arguments</p> <ul> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss | None</code>: A Keras loss function. Defaults to None. If None, the loss will be     inferred from the AutoModel.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to None. If None, the metrics     will be inferred from the AutoModel.</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We track the performance of the latest AutoKeras release on the benchmark datasets. Tested on a single NVIDIA Tesla V100 GPU.</p> Name API Metric Results GPU Days MNIST ImageClassifier Accuracy 99.04% 0.51 CIFAR10 ImageClassifier Accuracy 97.10% 1.8 IMDB TextClassifier Accuracy 93.93% 1.2"},{"location":"block/","title":"Block","text":"<p>[source]</p>"},{"location":"block/#convblock","title":"ConvBlock","text":"<pre><code>autokeras.ConvBlock(\n    kernel_size=None,\n    num_blocks=None,\n    num_layers=None,\n    filters=None,\n    max_pooling=None,\n    separable=None,\n    dropout=None,\n    **kwargs\n)\n</code></pre> <p>Block for vanilla ConvNets.</p> <p>Arguments</p> <ul> <li>kernel_size <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The size of the kernel.     If left unspecified, it will be tuned automatically.</li> <li>num_blocks <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of conv blocks, each of which may contain     convolutional, max pooling, dropout, and activation. If left     unspecified, it will be tuned automatically.</li> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or hyperparameters.Choice.     The number of convolutional layers in each block. If left     unspecified, it will be tuned automatically.</li> <li>filters <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice. The number of     filters in the convolutional layers. If left unspecified, it will     be tuned automatically.</li> <li>max_pooling <code>bool | None</code>: Boolean. Whether to use max pooling layer in each block. If     left unspecified, it will be tuned automatically.</li> <li>separable <code>bool | None</code>: Boolean. Whether to use separable conv layers.     If left unspecified, it will be tuned automatically.</li> <li>dropout <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or kerastuner.engine.hyperparameters.     Choice range Between 0 and 1.     The dropout rate after convolutional layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#denseblock","title":"DenseBlock","text":"<pre><code>autokeras.DenseBlock(num_layers=None, num_units=None, use_batchnorm=None, dropout=None, **kwargs)\n</code></pre> <p>Block for Dense layers.</p> <p>Arguments</p> <ul> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of Dense layers in the block.     If left unspecified, it will be tuned automatically.</li> <li>num_units <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of units in each dense layer.     If left unspecified, it will be tuned automatically.</li> <li>use_bn: Boolean. Whether to use BatchNormalization layers.     If left unspecified, it will be tuned automatically.</li> <li>dropout <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or keras_tuner.engine.hyperparameters.Choice.     The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#merge","title":"Merge","text":"<pre><code>autokeras.Merge(merge_type=None, **kwargs)\n</code></pre> <p>Merge block to merge multiple nodes into one.</p> <p>Arguments</p> <ul> <li>merge_type <code>str | None</code>: String. 'add' or 'concatenate'. If left unspecified, it will     be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#resnetblock","title":"ResNetBlock","text":"<pre><code>autokeras.ResNetBlock(version=None, pretrained=None, **kwargs)\n</code></pre> <p>Block for ResNet.</p> <p>Arguments</p> <ul> <li>version <code>str | None</code>: String. 'v1', 'v2'. The type of ResNet to use.     If left unspecified, it will be tuned automatically.</li> <li>pretrained <code>bool | None</code>: Boolean. Whether to use ImageNet pretrained weights.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#rnnblock","title":"RNNBlock","text":"<pre><code>autokeras.RNNBlock(\n    return_sequences=False, bidirectional=None, num_layers=None, layer_type=None, **kwargs\n)\n</code></pre> <p>An RNN Block.</p> <p>Arguments</p> <ul> <li>return_sequences <code>bool</code>: Boolean. Whether to return the last output in the     output sequence, or the full sequence. Defaults to False.</li> <li>bidirectional <code>bool | keras_tuner.src.engine.hyperparameters.hp_types.boolean_hp.Boolean | None</code>: Boolean or keras_tuner.engine.hyperparameters.Boolean.     Bidirectional RNN. If left unspecified, it will be     tuned automatically.</li> <li>num_layers <code>int | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Int or keras_tuner.engine.hyperparameters.Choice.     The number of layers in RNN. If left unspecified, it will     be tuned automatically.</li> <li>layer_type <code>str | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: String or or keras_tuner.engine.hyperparameters.Choice.     'gru' or 'lstm'. If left unspecified, it will be tuned     automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#spatialreduction","title":"SpatialReduction","text":"<pre><code>autokeras.SpatialReduction(reduction_type=None, **kwargs)\n</code></pre> <p>Reduce the dimension of a spatial tensor, e.g. image, to a vector.</p> <p>Arguments</p> <ul> <li>reduction_type <code>str | None</code>: String. 'flatten', 'global_max' or 'global_avg'.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#temporalreduction","title":"TemporalReduction","text":"<pre><code>autokeras.TemporalReduction(reduction_type=None, **kwargs)\n</code></pre> <p>Reduce the dim of a temporal tensor, e.g. output of RNN, to a vector.</p> <p>Arguments</p> <ul> <li>reduction_type <code>str | None</code>: String. 'flatten', 'global_max' or 'global_avg'. If left     unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#xceptionblock","title":"XceptionBlock","text":"<pre><code>autokeras.XceptionBlock(pretrained=None, **kwargs)\n</code></pre> <p>Block for XceptionNet.</p> <p>An Xception structure, used for specifying your model with specific datasets.</p> <p>The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow.</p> <p>This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by <code>HyperParameters</code>, to get an architecture with a half, an identical, or a double size of the original one.</p> <p>Arguments</p> <ul> <li>pretrained <code>bool | None</code>: Boolean. Whether to use ImageNet pretrained weights.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#imageblock","title":"ImageBlock","text":"<pre><code>autokeras.ImageBlock(block_type=None, normalize=None, augment=None, **kwargs)\n</code></pre> <p>Block for image data.</p> <p>The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'.</p> <p>Arguments</p> <ul> <li>block_type <code>str | None</code>: String. 'resnet', 'xception', 'vanilla'. The type of Block     to use. If unspecified, it will be tuned automatically.</li> <li>normalize <code>bool | None</code>: Boolean. Whether to channel-wise normalize the images.     If unspecified, it will be tuned automatically.</li> <li>augment <code>bool | None</code>: Boolean. Whether to do image augmentation. If unspecified,     it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#textblock","title":"TextBlock","text":"<pre><code>autokeras.TextBlock(**kwargs)\n</code></pre> <p>Block for text data.</p> <p>[source]</p>"},{"location":"block/#imageaugmentation","title":"ImageAugmentation","text":"<pre><code>autokeras.ImageAugmentation(\n    translation_factor=None,\n    vertical_flip=None,\n    horizontal_flip=None,\n    rotation_factor=None,\n    zoom_factor=None,\n    contrast_factor=None,\n    **kwargs\n)\n</code></pre> <p>Collection of various image augmentation methods.</p> <p>Arguments</p> <ul> <li>translation_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction value, or a     tuple of 2 representing fraction for translation vertically and     horizontally, or a kerastuner.engine.hyperparameters.Choice range     of positive floats. For instance, <code>translation_factor=0.2</code> result     in a random translation factor within 20% of the width and height.     If left unspecified, it will be tuned automatically.</li> <li>vertical_flip <code>bool | None</code>: Boolean. Whether to flip the image vertically.     If left unspecified, it will be tuned automatically.</li> <li>horizontal_flip <code>bool | None</code>: Boolean. Whether to flip the image horizontally.     If left unspecified, it will be tuned automatically.</li> <li>rotation_factor <code>float | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: Float or kerastuner.engine.hyperparameters.Choice range     between [0, 1]. A positive float represented as fraction of 2pi     upper bound for rotating clockwise and counter-clockwise. When     represented as a single float, lower = upper.     If left unspecified, it will be tuned automatically.</li> <li>zoom_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction value, or a tuple     of 2 representing fraction for zooming vertically and horizontally,     or a kerastuner.engine.hyperparameters.Choice range of positive     floats.  For instance, <code>zoom_factor=0.2</code> result in a random zoom     factor from 80% to 120%. If left unspecified, it will be tuned     automatically.</li> <li>contrast_factor <code>float | Tuple[float, float] | keras_tuner.src.engine.hyperparameters.hp_types.choice_hp.Choice | None</code>: A positive float represented as fraction of value, or a     tuple of size 2 representing lower and upper bound, or a     kerastuner.engine.hyperparameters.Choice range of floats to find the     optimal value. When represented as a single float, lower = upper.     The contrast factor will be randomly picked     between [1.0 - lower, 1.0 + upper]. If left unspecified, it will be     tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#normalization","title":"Normalization","text":"<pre><code>autokeras.Normalization(axis=-1, **kwargs)\n</code></pre> <p>Perform feature-wise normalization on data.</p> <p>Refer to Normalization layer in keras preprocessing layers for more information.</p> <p>Arguments</p> <ul> <li>axis <code>int</code>: Integer or tuple of integers, the axis or axes that should be     normalized (typically the features axis). We will normalize each     element in the specified axis. The default is '-1' (the innermost     axis); 0 (the batch axis) is not allowed.</li> </ul> <p>[source]</p>"},{"location":"block/#classificationhead","title":"ClassificationHead","text":"<pre><code>autokeras.ClassificationHead(\n    num_classes=None, multi_label=False, loss=None, metrics=None, dropout=None, **kwargs\n)\n</code></pre> <p>Classification Dense layers.</p> <p>Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default.</p> <p>The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be raw labels, one-hot encoded if more than two classes, or binary encoded for binary classification.</p> <p>The raw labels will be encoded to one column if two classes were found, or one-hot encoded if more than two classes were found.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss | None</code>: A Keras loss function. Defaults to use <code>binary_crossentropy</code> or     <code>categorical_crossentropy</code> based on the number of classes.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>dropout <code>float | None</code>: Float. The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul> <p>[source]</p>"},{"location":"block/#regressionhead","title":"RegressionHead","text":"<pre><code>autokeras.RegressionHead(\n    output_dim=None, loss=\"mean_squared_error\", metrics=None, dropout=None, **kwargs\n)\n</code></pre> <p>Regression Dense layers.</p> <p>The targets passing to the head would have to be tf.data.Dataset, np.ndarray, pd.DataFrame or pd.Series. It can be single-column or multi-column. The values should all be numerical.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>multi_label: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss</code>: A Keras loss function. Defaults to use <code>mean_squared_error</code>.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use <code>mean_squared_error</code>.</li> <li>dropout <code>float | None</code>: Float. The dropout rate for the layers.     If left unspecified, it will be tuned automatically.</li> </ul>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Contributions are welcome, and greatly appreciated! This page is only a guide of the best practices of contributing code to AutoKeras. The best way to contribute is to join our community by reading this. We will get you started right away.</p> <p>Follow the tag of good first issue for the issues for beginner.</p>"},{"location":"contributing/#pull-request-guide","title":"Pull Request Guide","text":"<ol> <li> <p>Is this the first pull request that you're making with GitHub? If so, read the guide Making a pull request to an open-source project.</p> </li> <li> <p>Include \"resolves #issue_number\" in the description of the pull request if applicable. Briefly describe your contribution.</p> </li> <li> <p>Submit the pull request from the first day of your development and create it as a draft pull request. Click <code>ready for review</code> when finished and passed the all the checks.</p> </li> <li> <p>For the case of bug fixes, add new test cases which would fail before your bug fix.</p> </li> </ol>"},{"location":"contributing/#setup-environment","title":"Setup Environment","text":"<p>We introduce 3 different options: GitHub Codespaces, VS Code &amp; Dev Containers, the general setup. You can choose base on your preference.</p>"},{"location":"contributing/#option-1-github-codespaces","title":"Option 1: GitHub Codespaces","text":"<p>You can simply open the repository in GitHub Codespaces. The environment is already setup there.</p>"},{"location":"contributing/#option-2-vs-code-dev-containers","title":"Option 2: VS Code &amp; Dev Containers","text":"<p>Open VS Code. Install the <code>Dev Containers</code> extension. Press <code>F1</code> key. Enter <code>Dev Containers: Open Folder in Container...</code> to open the repository root folder. The environment is already setup there.</p>"},{"location":"contributing/#option-3-the-general-setup","title":"Option 3: The General Setup","text":"<p>Install Virtualenvwrapper. Create a new virtualenv named <code>ak</code> based on python3. <pre><code>mkvirtualenv -p python3 ak \n</code></pre> Please use this virtualenv for development.</p> <p>Clone the repo. Go to the repo directory. Run the following commands. <pre><code>workon ak\n\npip install -e \".[tests]\"\npip uninstall autokeras\nadd2virtualenv .\n</code></pre></p>"},{"location":"contributing/#run-tests","title":"Run Tests","text":""},{"location":"contributing/#github-codespaces-or-vs-code-dev-containers","title":"GitHub Codespaces or VS Code &amp; Dev Containers","text":"<p>If you are using \"GitHub Codespaces\" or \"VS Code &amp; Dev Containers\", you can simply open any <code>*_test.py</code> file under the <code>tests</code> directory, and wait a few seconds, you will see the test tab on the left of the window.</p>"},{"location":"contributing/#general-setup","title":"General Setup","text":"<p>If you are using the general setup.</p> <p>Activate the virtualenv. Go to the repo directory Run the following lines to run the tests.</p> <p>Run all the tests. <pre><code>pytest tests\n</code></pre></p> <p>Run all the unit tests. <pre><code>pytest tests/autokeras\n</code></pre></p> <p>Run all the integration tests. <pre><code>pytest tests/integration_tests\n</code></pre></p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>You can run the following manually every time you want to format your code. 1. Run <code>shell/format.sh</code> to format your code. 2. Run <code>shell/lint.sh</code> to check.</p>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Docstrings should follow our style. Just check the style of other docstrings in AutoKeras.</p>"},{"location":"docker/","title":"Auto-Keras Docker","text":""},{"location":"docker/#download-auto-keras-docker-image","title":"Download Auto-Keras Docker image","text":"<p>The following command download Auto-Keras docker image to your machine.  </p> <pre><code>docker pull haifengjin/autokeras:latest\n</code></pre> <p>Image releases are tagged using the following format:</p> Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository"},{"location":"docker/#start-auto-keras-docker-container","title":"Start Auto-Keras Docker container","text":"<pre><code>docker run -it --shm-size 2G haifengjin/autokeras /bin/bash\n</code></pre> <p>In case you need more memory to run the container, change the value of <code>shm-size</code>. (Docker run reference)</p>"},{"location":"docker/#run-application","title":"Run application :","text":"<p>To run a local script <code>file.py</code> using Auto-Keras within the container, mount the host directory <code>-v hostDir:/app</code>.</p> <pre><code>docker run -it -v hostDir:/app --shm-size 2G haifengjin/autokeras python file.py\n</code></pre>"},{"location":"docker/#example","title":"Example :","text":"<p>Let's download the mnist example and run it within the container.  </p> <p>Download the example : <pre><code>curl https://raw.githubusercontent.com/keras-team/autokeras/master/examples/mnist.py --output mnist.py\n</code></pre></p> <p>Run the mnist example : <pre><code>docker run -it -v \"$(pwd)\":/app --shm-size 2G haifengjin/autokeras python /app/mnist.py\n</code></pre></p>"},{"location":"image_classifier/","title":"ImageClassifier","text":"<p>[source]</p>"},{"location":"image_classifier/#imageclassifier","title":"ImageClassifier","text":"<pre><code>autokeras.ImageClassifier(\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"image_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras image classification class.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss | None</code>: A Keras loss function. Defaults to use 'binary_crossentropy' or     'categorical_crossentropy' based on the number of classes.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'image_classifier'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs.  Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"image_classifier/#fit","title":"fit","text":"<pre><code>ImageClassifier.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x <code>numpy.ndarray | tensorflow.data.Dataset | None</code>: numpy.ndarray or tensorflow.Dataset. Training data x. The shape     of the data should be (samples, width, height) or (samples,     width, height, channels).</li> <li>y <code>numpy.ndarray | tensorflow.data.Dataset | None</code>: numpy.ndarray or tensorflow.Dataset. Training data y. It can be     raw labels, one-hot encoded if more than two classes, or binary     encoded for binary classification.</li> <li>epochs <code>int | None</code>: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks <code>List[tensorflow.keras.callbacks.Callback] | None</code>: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split <code>float | None</code>: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire dataset     including the validation data.</li> <li>validation_data <code>tensorflow.data.Dataset | Tuple[numpy.ndarray | tensorflow.data.Dataset, numpy.ndarray | tensorflow.data.Dataset] | None</code>: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training loss     values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"image_classifier/#predict","title":"predict","text":"<pre><code>ImageClassifier.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"image_classifier/#evaluate","title":"evaluate","text":"<pre><code>ImageClassifier.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"image_classifier/#export_model","title":"export_model","text":"<pre><code>ImageClassifier.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"image_regressor/","title":"ImageRegressor","text":"<p>[source]</p>"},{"location":"image_regressor/#imageregressor","title":"ImageRegressor","text":"<pre><code>autokeras.ImageRegressor(\n    output_dim=None,\n    loss=\"mean_squared_error\",\n    metrics=None,\n    project_name=\"image_regressor\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras image regression class.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss</code>: A Keras loss function. Defaults to use 'mean_squared_error'.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'mean_squared_error'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'image_regressor'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"image_regressor/#fit","title":"fit","text":"<pre><code>ImageRegressor.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x <code>numpy.ndarray | tensorflow.data.Dataset | None</code>: numpy.ndarray or tensorflow.Dataset. Training data x. The shape     of the data should be (samples, width, height) or (samples,     width, height, channels).</li> <li>y <code>numpy.ndarray | tensorflow.data.Dataset | None</code>: numpy.ndarray or tensorflow.Dataset. Training data y. The targets     passing to the head would have to be tf.data.Dataset,     np.ndarray, pd.DataFrame or pd.Series. It can be single-column     or multi-column.  The values should all be numerical.</li> <li>epochs <code>int | None</code>: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks <code>List[tensorflow.keras.callbacks.Callback] | None</code>: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split <code>float | None</code>: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset.  The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data <code>numpy.ndarray | tensorflow.data.Dataset | Tuple[numpy.ndarray | tensorflow.data.Dataset] | None</code>: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"image_regressor/#predict","title":"predict","text":"<pre><code>ImageRegressor.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"image_regressor/#evaluate","title":"evaluate","text":"<pre><code>ImageRegressor.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"image_regressor/#export_model","title":"export_model","text":"<pre><code>ImageRegressor.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#requirements","title":"Requirements","text":"<p>Python 3: Follow the TensorFlow install steps to install Python 3.</p> <p>Pip: Follow the TensorFlow install steps to install Pip.</p> <p>Tensorflow &gt;= 2.3.0: AutoKeras is based on TensorFlow. Please follow this tutorial to install TensorFlow for python3.</p> <p>GPU Setup (Optional): If you have GPUs on your machine and want to use them to accelerate the training, you can follow this tutorial to setup.</p>"},{"location":"install/#install-autokeras","title":"Install AutoKeras","text":"<p>AutoKeras only support Python 3. If you followed previous steps to use virtualenv to install tensorflow, you can just activate the virtualenv and use the following command to install AutoKeras.  <pre><code>pip install git+https://github.com/keras-team/keras-tuner.git\npip install autokeras\n</code></pre></p> <p>If you did not use virtualenv, and you use <code>python3</code> command to execute your python program, please use the following command to install AutoKeras. <pre><code>python3 -m pip install git+https://github.com/keras-team/keras-tuner.git\npython3 -m pip install autokeras\n</code></pre></p>"},{"location":"node/","title":"Node","text":"<p>[source]</p>"},{"location":"node/#imageinput","title":"ImageInput","text":"<pre><code>autokeras.ImageInput(name=None, **kwargs)\n</code></pre> <p>Input node for image data.</p> <p>The input data should be numpy.ndarray or tf.data.Dataset. The shape of the data should be should be (samples, width, height) or (samples, width, height, channels).</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul> <p>[source]</p>"},{"location":"node/#input","title":"Input","text":"<pre><code>autokeras.Input(name=None, **kwargs)\n</code></pre> <p>Input node for tensor data.</p> <p>The data should be numpy.ndarray or tf.data.Dataset.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul> <p>[source]</p>"},{"location":"node/#textinput","title":"TextInput","text":"<pre><code>autokeras.TextInput(name=None, **kwargs)\n</code></pre> <p>Input node for text data.</p> <p>The input data should be numpy.ndarray or tf.data.Dataset. The data should be one-dimensional. Each element in the data should be a string which is a full sentence.</p> <p>Arguments</p> <ul> <li>name <code>str | None</code>: String. The name of the input node. If unspecified, it will be set     automatically with the class name.</li> </ul>"},{"location":"text_classifier/","title":"TextClassifier","text":"<p>[source]</p>"},{"location":"text_classifier/#textclassifier","title":"TextClassifier","text":"<pre><code>autokeras.TextClassifier(\n    num_classes=None,\n    multi_label=False,\n    loss=None,\n    metrics=None,\n    project_name=\"text_classifier\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras text classification class.</p> <p>Arguments</p> <ul> <li>num_classes <code>int | None</code>: Int. Defaults to None. If None, it will be inferred from     the data.</li> <li>multi_label <code>bool</code>: Boolean. Defaults to False.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss | None</code>: A Keras loss function. Defaults to use 'binary_crossentropy' or     'categorical_crossentropy' based on the number of classes.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'accuracy'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'text_classifier'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"text_classifier/#fit","title":"fit","text":"<pre><code>TextClassifier.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray or tensorflow.Dataset. Training data x. The input     data should be numpy.ndarray or tf.data.Dataset. The data should     be one dimensional. Each element in the data should be a string     which is a full sentence.</li> <li>y: numpy.ndarray or tensorflow.Dataset. Training data y. It can be     raw labels, one-hot encoded if more than two classes, or binary     encoded for binary classification.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data.  The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"text_classifier/#predict","title":"predict","text":"<pre><code>TextClassifier.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"text_classifier/#evaluate","title":"evaluate","text":"<pre><code>TextClassifier.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"text_classifier/#export_model","title":"export_model","text":"<pre><code>TextClassifier.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"text_regressor/","title":"TextRegressor","text":"<p>[source]</p>"},{"location":"text_regressor/#textregressor","title":"TextRegressor","text":"<pre><code>autokeras.TextRegressor(\n    output_dim=None,\n    loss=\"mean_squared_error\",\n    metrics=None,\n    project_name=\"text_regressor\",\n    max_trials=100,\n    directory=None,\n    objective=\"val_loss\",\n    tuner=None,\n    overwrite=False,\n    seed=None,\n    max_model_size=None,\n    **kwargs\n)\n</code></pre> <p>AutoKeras text regression class.</p> <p>Arguments</p> <ul> <li>output_dim <code>int | None</code>: Int. The number of output dimensions. Defaults to None.     If None, it will be inferred from the data.</li> <li>loss <code>str | Callable | tensorflow.keras.losses.Loss</code>: A Keras loss function. Defaults to use 'mean_squared_error'.</li> <li>metrics <code>List[str | Callable | tensorflow.keras.metrics.Metric] | List[List[str | Callable | tensorflow.keras.metrics.Metric]] | Dict[str, str | Callable | tensorflow.keras.metrics.Metric] | None</code>: A list of Keras metrics. Defaults to use 'mean_squared_error'.</li> <li>project_name <code>str</code>: String. The name of the AutoModel.     Defaults to 'text_regressor'.</li> <li>max_trials <code>int</code>: Int. The maximum number of different Keras Models to try.     The search may finish before reaching the max_trials. Defaults to     100.</li> <li>directory <code>str | pathlib.Path | None</code>: String. The path to a directory for storing the search     outputs. Defaults to None, which would create a folder with the     name of the AutoModel in the current directory.</li> <li>objective <code>str</code>: String. Name of model metric to minimize     or maximize, e.g. 'val_accuracy'. Defaults to 'val_loss'.</li> <li>tuner <code>str | Type[autokeras.engine.tuner.AutoTuner] | None</code>: String or subclass of AutoTuner. If string, it should be one of     'greedy', 'bayesian', 'hyperband' or 'random'. It can also be a     subclass of AutoTuner. If left unspecified, it uses a task specific     tuner, which first evaluates the most commonly used models for the     task before exploring other models.</li> <li>overwrite <code>bool</code>: Boolean. Defaults to <code>False</code>. If <code>False</code>, reloads an existing     project of the same name if one is found. Otherwise, overwrites the     project.</li> <li>seed <code>int | None</code>: Int. Random seed.</li> <li>max_model_size <code>int | None</code>: Int. Maximum number of scalars in the parameters of a     model. Models larger than this are rejected.</li> <li>**kwargs: Any arguments supported by AutoModel.</li> </ul> <p>[source]</p>"},{"location":"text_regressor/#fit","title":"fit","text":"<pre><code>TextRegressor.fit(\n    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs\n)\n</code></pre> <p>Search for the best model and hyperparameters for the AutoModel.</p> <p>It will search for the best model based on the performances on validation data.</p> <p>Arguments</p> <ul> <li>x: numpy.ndarray or tensorflow.Dataset. Training data x. The input     data should be numpy.ndarray or tf.data.Dataset. The data should     be one dimensional. Each element in the data should be a string     which is a full sentence.</li> <li>y: numpy.ndarray or tensorflow.Dataset. Training data y. The targets     passing to the head would have to be tf.data.Dataset,     np.ndarray, pd.DataFrame or pd.Series. It can be single-column     or multi-column.  The values should all be numerical.</li> <li>epochs: Int. The number of epochs to train each model during the     search. If unspecified, by default we train for a maximum of     1000 epochs, but we stop training if the validation loss stops     improving for 10 epochs (unless you specified an EarlyStopping     callback as part of the callbacks argument, in which case the     EarlyStopping callback you specified will determine early     stopping).</li> <li>callbacks: List of Keras callbacks to apply during training and     validation.</li> <li>validation_split: Float between 0 and 1. Defaults to 0.2. Fraction     of the training data to be used as validation data. The model     will set apart this fraction of the training data, will not     train on it, and will evaluate the loss and any model metrics on     this data at the end of each epoch. The validation data is     selected from the last samples in the <code>x</code> and <code>y</code> data provided,     before shuffling. This argument is not supported when <code>x</code> is a     dataset. The best model found would be fit on the entire     dataset including the validation data.</li> <li>validation_data: Data on which to evaluate the loss and any model     metrics at the end of each epoch. The model will not be trained     on this data. <code>validation_data</code> will override     <code>validation_split</code>. The type of the validation data should be     the same as the training data. The best model found would be     fit on the training dataset without the validation data.</li> <li>**kwargs: Any arguments supported by     keras.Model.fit.</li> </ul> <p>Returns</p> <p>history: A Keras History object corresponding to the best model.     Its History.history attribute is a record of training     loss values and metrics values at successive epochs, as well as     validation loss values and validation metrics values (if     applicable).</p> <p>[source]</p>"},{"location":"text_regressor/#predict","title":"predict","text":"<pre><code>TextRegressor.predict(x, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Predict the output for a given testing data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.predict</li> <li>**kwargs: Any arguments supported by keras.Model.predict.</li> </ul> <p>Returns</p> <p>A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.</p> <p>[source]</p>"},{"location":"text_regressor/#evaluate","title":"evaluate","text":"<pre><code>TextRegressor.evaluate(x, y=None, batch_size=32, verbose=1, **kwargs)\n</code></pre> <p>Evaluate the best model for the given data.</p> <p>Arguments</p> <ul> <li>x: Any allowed types according to the input node. Testing data.</li> <li>y: Any allowed types according to the head. Testing targets.     Defaults to None.</li> <li>batch_size: Number of samples per batch.     If unspecified, batch_size will default to 32.</li> <li>verbose: Verbosity mode. 0 = silent, 1 = progress bar.     Controls the verbosity of     keras.Model.evaluate</li> <li>**kwargs: Any arguments supported by keras.Model.evaluate.</li> </ul> <p>Returns</p> <p>Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs.</p> <p>[source]</p>"},{"location":"text_regressor/#export_model","title":"export_model","text":"<pre><code>TextRegressor.export_model()\n</code></pre> <p>Export the best Keras Model.</p> <p>Returns</p> <p>keras.Model instance. The best model found during the search, loaded with trained weights.</p>"},{"location":"utils/","title":"Utils","text":"<p>[source]</p>"},{"location":"utils/#image_dataset_from_directory","title":"image_dataset_from_directory","text":"<pre><code>autokeras.image_dataset_from_directory(\n    directory,\n    batch_size=32,\n    color_mode=\"rgb\",\n    image_size=(256, 256),\n    interpolation=\"bilinear\",\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n)\n</code></pre> <p>Generates a <code>tf.data.Dataset</code> from image files in a directory. If your directory structure is:</p> <pre><code>main_directory/\n...class_a/\n......a_image_1.jpg\n......a_image_2.jpg\n...class_b/\n......b_image_1.jpg\n......b_image_2.jpg\n</code></pre> <p>Then calling <code>image_dataset_from_directory(main_directory)</code> will return a <code>tf.data.Dataset</code> that yields batches of images from the subdirectories <code>class_a</code> and <code>class_b</code>, together with labels 'class_a' and 'class_b'.</p> <p>Supported image formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame.</p> <p>Arguments</p> <ul> <li>directory <code>str</code>: Directory where the data is located.     If <code>labels</code> is \"inferred\", it should contain     subdirectories, each containing images for a class.     Otherwise, the directory structure is ignored.</li> <li>batch_size <code>int</code>: Size of the batches of data. Default: 32.</li> <li>color_mode <code>str</code>: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".     Whether the images will be converted to     have 1, 3, or 4 channels.</li> <li>image_size <code>Tuple[int, int]</code>: Size to resize images to after they are read from disk.     Defaults to <code>(256, 256)</code>.     Since the pipeline processes batches of images that must all have     the same size, this must be provided.</li> <li>interpolation <code>str</code>: String, the interpolation method used when resizing     images. Defaults to <code>bilinear</code>. Supports <code>bilinear</code>, <code>nearest</code>,     <code>bicubic</code>, <code>area</code>, <code>lanczos3</code>, <code>lanczos5</code>, <code>gaussian</code>,     <code>mitchellcubic</code>.</li> <li>shuffle <code>bool</code>: Whether to shuffle the data. Default: True.     If set to False, sorts the data in alphanumeric order.</li> <li>seed <code>int | None</code>: Optional random seed for shuffling and transformations.</li> <li>validation_split <code>float | None</code>: Optional float between 0 and 1,     fraction of data to reserve for validation.</li> <li>subset <code>str | None</code>: One of \"training\" or \"validation\".     Only used if <code>validation_split</code> is set.</li> </ul> <p>Returns</p> <p>A <code>tf.data.Dataset</code> object, which yields a tuple <code>(texts, labels)</code>, where <code>images</code> has shape <code>(batch_size, image_size[0], image_size[1], num_channels)</code> where <code>labels</code> has shape <code>(batch_size,)</code> and type of tf.string. - if <code>color_mode</code> is <code>grayscale</code>, there's 1 channel in the image tensors. - if <code>color_mode</code> is <code>rgb</code>, there are 3 channel in the image tensors. - if <code>color_mode</code> is <code>rgba</code>, there are 4 channel in the image tensors.</p> <p>[source]</p>"},{"location":"utils/#text_dataset_from_directory","title":"text_dataset_from_directory","text":"<pre><code>autokeras.text_dataset_from_directory(\n    directory, batch_size=32, max_length=None, shuffle=True, seed=None, validation_split=None, subset=None\n)\n</code></pre> <p>Generates a <code>tf.data.Dataset</code> from text files in a directory.</p> <p>If your directory structure is:</p> <pre><code>main_directory/\n...class_a/\n......a_text_1.txt\n......a_text_2.txt\n...class_b/\n......b_text_1.txt\n......b_text_2.txt\n</code></pre> <p>Then calling <code>text_dataset_from_directory(main_directory)</code> will return a <code>tf.data.Dataset</code> that yields batches of texts from the subdirectories <code>class_a</code> and <code>class_b</code>, together with labels 'class_a' and 'class_b'.</p> <p>Only <code>.txt</code> files are supported at this time.</p> <p>Arguments</p> <ul> <li>directory <code>str</code>: Directory where the data is located.     If <code>labels</code> is \"inferred\", it should contain     subdirectories, each containing text files for a class.     Otherwise, the directory structure is ignored.</li> <li>batch_size <code>int</code>: Size of the batches of data. Defaults to 32.</li> <li>max_length <code>int | None</code>: Maximum size of a text string. Texts longer than this will     be truncated to <code>max_length</code>.</li> <li>shuffle <code>bool</code>: Whether to shuffle the data. Default: True.     If set to False, sorts the data in alphanumeric order.</li> <li>seed <code>int | None</code>: Optional random seed for shuffling and transformations.</li> <li>validation_split <code>float | None</code>: Optional float between 0 and 1,     fraction of data to reserve for validation.</li> <li>subset <code>str | None</code>: One of \"training\" or \"validation\".     Only used if <code>validation_split</code> is set.</li> </ul> <p>Returns</p> <p>A <code>tf.data.Dataset</code> object, which yields a tuple <code>(texts, labels)</code>,     where both has shape <code>(batch_size,)</code> and type of tf.string.</p>"},{"location":"examples/automodel_with_cnn/","title":"Automodel with cnn","text":"<pre><code>import keras\nimport numpy as np\n\nimport autokeras as ak\n\n# Prepare example Data - Shape 1D\nnum_instances = 100\nnum_features = 5\nx_train = np.random.rand(num_instances, num_features).astype(np.float32)\ny_train = np.zeros(num_instances).astype(np.float32)\ny_train[0 : int(num_instances / 2)] = 1\nx_test = np.random.rand(num_instances, num_features).astype(np.float32)\ny_test = np.zeros(num_instances).astype(np.float32)\ny_train[0 : int(num_instances / 2)] = 1\n\nx_train = np.expand_dims(\n    x_train, axis=2\n)  # This step it's very important an CNN will only accept this data shape\nprint(x_train.shape)\nprint(y_train.shape)\n\n\n# Prepare Automodel for search\ninput_node = ak.Input()\noutput_node = ak.ConvBlock()(input_node)\n# output_node = ak.DenseBlock()(output_node) #optional\n# output_node = ak.SpatialReduction()(output_node) #optional\noutput_node = ak.ClassificationHead(num_classes=2, multi_label=True)(\n    output_node\n)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\n\n\n# Search\nauto_model.fit(x_train, y_train, epochs=1)\nprint(auto_model.evaluate(x_test, y_test))\n\n\n# Export as a Keras Model\nmodel = auto_model.export_model()\nprint(type(model.summary()))\n\n# print model as image\nkeras.utils.plot_model(\n    model, show_shapes=True, expand_treeed=True, to_file=\"name.png\"\n)\n</code></pre>"},{"location":"examples/celeb_age/","title":"Celeb age","text":"<p>Regression tasks estimate a numeric variable, such as the price of a house or voter turnout.</p> <p>This example is adapted from a notebook which estimates a person's age from their image, trained on the IMDB-WIKI photographs of famous people.</p> <p>First, prepare your image data in a numpy.ndarray or tensorflow.Dataset format. Each image must have the same shape, meaning each has the same width, height, and color channels as other images in the set.</p> <pre><code>Regression tasks estimate a numeric variable, such as the price of a house or\nvoter turnout.\n\nThis example is adapted from a\n[notebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07)\nwhich estimates a person's age from their image, trained on the\n[IMDB-WIKI](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) photographs\nof famous\npeople.\n\nFirst, prepare your image data in a numpy.ndarray or tensorflow.Dataset format.\nEach image must have the same shape, meaning each has the same width, height,\nand color channels as other images in the set.\n\"\"\"\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom google.colab import drive\nfrom PIL import Image\nfrom scipy.io import loadmat\n\nimport autokeras as ak\n\n\"\"\"\n### Connect your Google Drive for Data\n\"\"\"\n\n\ndrive.mount(\"/content/drive\")\n\n\"\"\"\n### Install AutoKeras and TensorFlow\n\nDownload the master branch to your Google Drive for this tutorial. In general,\nyou can use *pip install autokeras* .\n\"\"\"\n\n\"\"\"shell\n!pip install  -v \"/content/drive/My Drive/AutoKeras-dev/autokeras-master.zip\"\n!pip uninstall keras-tuner\n!pip install\ngit+git://github.com/keras-team/keras-tuner.git@d2d69cba21a0b482a85ce2a38893e2322e139c01\n\"\"\"\n\n\"\"\"shell\n!pip install tensorflow==2.2.0\n\"\"\"\n\n\"\"\"\n###**Import IMDB Celeb images and metadata**\n\"\"\"\n\n\"\"\"shell\n!mkdir \"./drive/My Drive/mlin/celebs\"\n\"\"\"\n\n\"\"\"shell\n! wget -O \"./drive/My Drive/mlin/celebs/imdb_0.tar\"\nhttps://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_0.tar\n\"\"\"\n\n\"\"\"shell\n! cd \"./drive/My Drive/mlin/celebs\" &amp;&amp; tar -xf imdb_0.tar\n! rm \"./drive/My Drive/mlin/celebs/imdb_0.tar\"\n\"\"\"\n\n\"\"\"\nUncomment and run the below cell if you need to re-run the cells again and\nabove don't need to install everything from the beginning.\n\"\"\"\n\n# ! cd ./drive/My\\ Drive/mlin/celebs.\n\n\"\"\"shell\n! ls \"./drive/My Drive/mlin/celebs/imdb/\"\n\"\"\"\n\n\"\"\"shell\n! wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_meta.tar\n! tar -xf imdb_meta.tar\n! rm imdb_meta.tar\n\"\"\"\n\n\"\"\"\n###**Converting from MATLAB date to actual Date-of-Birth**\n\"\"\"\n\n\ndef datenum_to_datetime(datenum):\n    \"\"\"\n    Convert Matlab datenum into Python datetime.\n    \"\"\"\n    days = datenum % 1\n    hours = days % 1 * 24\n    minutes = hours % 1 * 60\n    seconds = minutes % 1 * 60\n    try:\n        return (\n            datetime.fromordinal(int(datenum))\n            + timedelta(days=int(days))\n            + timedelta(hours=int(hours))\n            + timedelta(minutes=int(minutes))\n            + timedelta(seconds=round(seconds))\n            - timedelta(days=366)\n        )\n    except Exception:\n        return datenum_to_datetime(700000)\n\n\nprint(datenum_to_datetime(734963))\n\n\"\"\"\n### **Opening MatLab file to Pandas DataFrame**\n\"\"\"\n\n\nx = loadmat(\"imdb/imdb.mat\")\n\n\nmdata = x[\"imdb\"]  # variable in mat file\nmdtype = mdata.dtype  # dtypes of structures are \"unsized objects\"\nndata = {n: mdata[n][0, 0] for n in mdtype.names}\ncolumns = [n for n, v in ndata.items()]\n\nrows = []\nfor col in range(0, 10):\n    values = list(ndata.items())[col]\n    for num, val in enumerate(values[1][0], start=0):\n        if col == 0:\n            rows.append([])\n        if num &gt; 0:\n            if columns[col] == \"dob\":\n                rows[num].append(datenum_to_datetime(int(val)))\n            elif columns[col] == \"photo_taken\":\n                rows[num].append(datetime(year=int(val), month=6, day=30))\n            else:\n                rows[num].append(val)\n\ndt = map(lambda row: np.array(row), np.array(rows[1:]))\n\ndf = pd.DataFrame(data=dt, index=range(0, len(rows) - 1), columns=columns)\nprint(df.head())\n\nprint(columns)\nprint(df[\"full_path\"])\n\n\"\"\"\n### **Calculating age at time photo was taken**\n\"\"\"\n\ndf[\"age\"] = (df[\"photo_taken\"] - df[\"dob\"]).astype(\"int\") / 31558102e9\nprint(df[\"age\"])\n\n\"\"\"\n### **Creating dataset**\n\n\n* We sample 200 of the images which were included in this first download.\n* Images are resized to 128x128 to standardize shape and conserve memory\n* RGB images are converted to grayscale to standardize shape\n* Ages are converted to ints\n\n\n\"\"\"\n\n\ndef df2numpy(train_set):\n    images = []\n    for img_path in train_set[\"full_path\"]:\n        img = (\n            Image.open(\"./drive/My Drive/mlin/celebs/imdb/\" + img_path[0])\n            .resize((128, 128))\n            .convert(\"L\")\n        )\n        images.append(np.asarray(img, dtype=\"int32\"))\n\n    image_inputs = np.array(images)\n\n    ages = train_set[\"age\"].astype(\"int\").to_numpy()\n    return image_inputs, ages\n\n\ntrain_set = df[df[\"full_path\"] &lt; \"02\"].sample(200)\ntrain_imgs, train_ages = df2numpy(train_set)\n\ntest_set = df[df[\"full_path\"] &lt; \"02\"].sample(100)\ntest_imgs, test_ages = df2numpy(test_set)\n\n\"\"\"\n### **Training using AutoKeras**\n\"\"\"\n\n\n# Initialize the image regressor\nreg = ak.ImageRegressor(max_trials=15)  # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(train_imgs, train_ages)\n\n# Predict with the chosen model:\n# predict_y = reg.predict(test_images)  # Uncomment if required\n\n# Evaluate the chosen model with testing data\nprint(reg.evaluate(train_imgs, train_ages))\n\n\"\"\"\n### **Validation Data**\n\nBy default, AutoKeras use the last 20% of training data as validation data. As\nshown in the example below, you can use validation_split to specify the\npercentage.\n\"\"\"\n\nreg.fit(\n    train_imgs,\n    train_ages,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=3,\n)\n\n\"\"\"\nYou can also use your own validation set instead of splitting it from the\ntraining data with validation_data.\n\"\"\"\n\nsplit = 460000\nx_val = train_imgs[split:]\ny_val = train_ages[split:]\nx_train = train_imgs[:split]\ny_train = train_ages[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=3,\n)\n\n\"\"\"\n### **Customized Search Space**\n\nFor advanced users, you may customize your search space by using AutoModel\ninstead of ImageRegressor. You can configure the ImageBlock for some high-level\nconfigurations, e.g., block_type for the type of neural network to search,\nnormalize for whether to do data normalization, augment for whether to do data\naugmentation. You can also choose not to specify these arguments, which would\nleave the different choices to be tuned automatically. See the following\nexample for detail.\n\"\"\"\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nreg.fit(x_train, y_train, epochs=3)\n\n\"\"\"\nThe usage of AutoModel is similar to the functional API of Keras. Basically, you\nare building a graph, whose edges are blocks and the nodes are intermediate\noutputs of blocks. To add an edge from input_node to output_node with\noutput_node = ak.some_block(input_node).  You can even also use more fine\ngrained blocks to customize the search space even further. See the following\nexample.\n\"\"\"\n\n\ninput_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(translation_factor=0.3)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.RegressionHead()(output_node)\nclf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=10)\nclf.fit(x_train, y_train, epochs=3)\n\n\"\"\"\n### **Data Format**\n\"\"\"\n\n\"\"\"\nThe AutoKeras ImageClassifier is quite flexible for the data format.\n\nFor the image, it accepts data formats both with and without the channel\ndimension. The images in the IMDB-Wiki dataset do not have a channel dimension.\nEach image is a matrix with shape (128, 128). AutoKeras also accepts images\nwith a channel dimension at last, e.g., (32, 32, 3), (28, 28, 1).\n\nFor the classification labels, AutoKeras accepts both plain labels, i.e.\nstrings or integers, and one-hot encoded labels, i.e. vectors of 0s and 1s.\n\nSo if you prepare your data in the following way, the ImageClassifier should\nstill work.\n\"\"\"\n\n# Reshape the images to have the channel dimension.\ntrain_imgs = train_imgs.reshape(train_imgs.shape + (1,))\ntest_imgs = test_imgs.reshape(test_imgs.shape + (1,))\n\nprint(train_imgs.shape)  # (200, 128, 128, 1)\nprint(test_imgs.shape)  # (100, 128, 128, 1)\nprint(train_ages[:3])\n\n\"\"\"\nWe also support using tf.data.Dataset format for the training data. In this\ncase, the images would have to be 3-dimentional. The labels have to be one-hot\nencoded for multi-class classification to be wrapped into tensorflow Dataset.\n\"\"\"\n\n\ntrain_set = tf.data.Dataset.from_tensor_slices(((train_imgs,), (train_ages,)))\ntest_set = tf.data.Dataset.from_tensor_slices(((test_imgs,), (test_ages,)))\n\nreg = ak.ImageRegressor(max_trials=15)\n# Feed the tensorflow Dataset to the classifier.\nreg.fit(train_set)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n\n\"\"\"\n## References\n\n[Main Reference\nNotebook](https://gist.github.com/mapmeld/98d1e9839f2d1f9c4ee197953661ed07),\n[Dataset](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/),\n[ImageRegressor](/image_regressor),\n[ResNetBlock](/block/#resnetblock-class),\n[ImageInput](/node/#imageinput-class),\n[AutoModel](/auto_model/#automodel-class),\n[ImageBlock](/block/#imageblock-class),\n[Normalization](/preprocessor/#normalization-class),\n[ImageAugmentation](/preprocessor/#image-augmentation-class),\n[RegressionHead](/head/#regressionhead-class).\n\n\"\"\"\n</code></pre>"},{"location":"examples/cifar10/","title":"Cifar10","text":"<pre><code>import autokeras as ak\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=5)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)[1]))\n</code></pre>"},{"location":"examples/imdb/","title":"Imdb","text":"<p>Search for a good model for the IMDB dataset.</p> <pre><code>Search for a good model for the\n[IMDB](\nhttps://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) dataset.\n\"\"\"\n\nimport keras\nimport numpy as np\n\nimport autokeras as ak\n\n\ndef imdb_raw():\n    max_features = 20000\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n        num_words=max_features, index_from=index_offset\n    )\n    x_train = x_train\n    y_train = y_train.reshape(-1, 1)\n    x_test = x_test\n    y_test = y_test.reshape(-1, 1)\n\n    word_to_id = keras.datasets.imdb.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[\"&lt;PAD&gt;\"] = 0\n    word_to_id[\"&lt;START&gt;\"] = 1\n    word_to_id[\"&lt;UNK&gt;\"] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train)\n    )\n    x_test = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test)\n    )\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Prepare the data.\n(x_train, y_train), (x_test, y_test) = imdb_raw()\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # &lt;START&gt; this film was just brilliant casting &lt;UNK&gt;\n\n# Initialize the TextClassifier\nclf = ak.TextClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=2, batch_size=8)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"examples/mnist/","title":"Mnist","text":"<p>Search for a good model for the MNIST dataset.</p> <pre><code>Search for a good model for the\n[MNIST](https://keras.io/datasets/#mnist-database-of-handwritten-digits)\ndataset.\n\"\"\"\n\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=3)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"examples/new_pop/","title":"New pop","text":"<p>shell pip install autokeras</p> <pre><code>pip install autokeras\n\"\"\"\n\nimport pandas as pd\n\nimport autokeras as ak\n\n\"\"\"\n## Social Media Articles Example\n\nRegression tasks estimate a numeric variable, such as the price of a house\nor a person's age.\n\nThis example estimates the view counts for an article on social media platforms,\ntrained on a\n[News Popularity](\nhttps://archive.ics.uci.edu/ml/datasets/\nNews+Popularity+in+Multiple+Social+Media+Platforms)\ndataset collected from 2015-2016.\n\nFirst, prepare your text data in a `numpy.ndarray` or `tensorflow.Dataset`\nformat.\n\"\"\"\n\n\n# converting from other formats (such as pandas) to numpy\ndf = pd.read_csv(\"./News_Final.csv\")\n\ntext_inputs = df.Title.to_numpy(dtype=\"str\")\nmedia_success_outputs = df.Facebook.to_numpy(dtype=\"int\")\n\n\"\"\"\nNext, initialize and train the [TextRegressor](/text_regressor).\n\"\"\"\n\n\n# Initialize the text regressor\nreg = ak.TextRegressor(max_trials=15)  # AutoKeras tries 15 different models.\n\n# Find the best model for the given training data\nreg.fit(text_inputs, media_success_outputs)\n\n# Predict with the chosen model:\npredict_y = reg.predict(text_inputs)\n</code></pre>"},{"location":"examples/reuters/","title":"Reuters","text":"<p>shell !pip install -q -U pip !pip install -q -U autokeras==1.0.8 !pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1</p> <pre><code>!pip install -q -U pip\n!pip install -q -U autokeras==1.0.8\n!pip install -q git+https://github.com/keras-team/keras-tuner.git@1.0.2rc1\n\"\"\"\n\nimport keras\nimport numpy as np\nfrom keras.datasets import reuters\n\nimport autokeras as ak\n\n\"\"\"\nSearch for a good model for the\n[Reuters](https://keras.io/ja/datasets/#_5) dataset.\n\"\"\"\n\n\n# Prepare the dataset.\ndef reuters_raw(max_features=20000):\n    index_offset = 3  # word index offset\n\n    (x_train, y_train), (x_test, y_test) = reuters.load_data(\n        num_words=max_features, index_from=index_offset\n    )\n    x_train = x_train\n    y_train = y_train.reshape(-1, 1)\n    x_test = x_test\n    y_test = y_test.reshape(-1, 1)\n\n    word_to_id = reuters.get_word_index()\n    word_to_id = {k: (v + index_offset) for k, v in word_to_id.items()}\n    word_to_id[\"&lt;PAD&gt;\"] = 0\n    word_to_id[\"&lt;START&gt;\"] = 1\n    word_to_id[\"&lt;UNK&gt;\"] = 2\n\n    id_to_word = {value: key for key, value in word_to_id.items()}\n    x_train = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_train)\n    )\n    x_test = list(\n        map(lambda sentence: \" \".join(id_to_word[i] for i in sentence), x_test)\n    )\n    x_train = np.array(x_train, dtype=np.str)\n    x_test = np.array(x_test, dtype=np.str)\n    return (x_train, y_train), (x_test, y_test)\n\n\n# Prepare the data.\n(x_train, y_train), (x_test, y_test) = reuters_raw()\nprint(x_train.shape)  # (8982,)\nprint(y_train.shape)  # (8982, 1)\nprint(x_train[0][:50])  # &lt;START&gt; &lt;UNK&gt; &lt;UNK&gt; said as a result of its decemb\n\n# Initialize the TextClassifier\nclf = ak.TextClassifier(\n    max_trials=5,\n    overwrite=True,\n)\n\n# Callback to avoid overfitting with the EarlyStopping.\ncbs = [\n    keras.callbacks.EarlyStopping(patience=3),\n]\n\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10, callback=cbs)\n\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)))\n</code></pre>"},{"location":"extensions/tf_cloud/","title":"TensorFlow Cloud","text":"<p>TensorFlow Cloud allows you to run your  TensorFlow program leveraging the computing power on Google Cloud easily. Please follow the instructions to setup your account.</p> <p>AutoKeras has successfully integrated with this service. Now you can run your program on Google Cloud only by inserting a few more lines of code. Please see the example below.</p> <p><pre><code>import argparse\nimport os\n\nimport autokeras as ak\nimport tensorflow_cloud as tfc\nfrom tensorflow.keras.datasets import mnist\n\n\nparser = argparse.ArgumentParser(description=\"Model save path arguments.\")\nparser.add_argument(\"--path\", required=True, type=str, help=\"Keras model save path\")\nargs = parser.parse_args()\n\ntfc.run(\n    chief_config=tfc.COMMON_MACHINE_CONFIGS[\"V100_1X\"],\n    docker_base_image=\"haifengjin/autokeras:1.0.3\",\n)\n\n# Prepare the dataset.\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Initialize the ImageClassifier.\nclf = ak.ImageClassifier(max_trials=2)\n# Search for the best model.\nclf.fit(x_train, y_train, epochs=10)\n# Evaluate on the testing data.\nprint(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)[1]))\n\nclf.export_model().save(os.path.join(args.path, \"model.h5\"))\n</code></pre> You can find the code above here</p>"},{"location":"extensions/trains/","title":"Trains Integration","text":"<p>Allegro Trains is a full system open source ML / DL experiment manager and ML-Ops solution. It enables data scientists and data engineers to effortlessly track, manage, compare and collaborate on their experiments as well as easily manage their training workloads on remote machines.</p> <p>Trains is a suite of open source Python packages and plugins, including:</p> <ul> <li>Trains Python Client package - Integrate Trains into your AutoKeras tasks with just two lines of code, and get all of Trains robust features. </li> <li>Trains Server - The Trains backend infrastructure and web UI. Use the public Trains demo server, or deploy your own.</li> <li>Trains Agent -  The Trains DevOps component for experiment execution, resource control, and autoML..</li> <li>Additional integrations - Integrate Trains with PyCharm and Jupyter Notebook. </li> </ul> <p></p>"},{"location":"extensions/trains/#setting-up-trains","title":"Setting up Trains","text":"<p>To integrate Trains into your AutoKeras project, do the following:</p> <ol> <li> <p>Install the Trains Python Client package.</p> <pre><code>pip install trains\n</code></pre> </li> <li> <p>Add the short Trains initialization code to your task.</p> <pre><code>from trains import Task\n\ntask = Task.init(project_name=\"autokeras\", task_name=\"autokeras imdb example with scalars\")\n</code></pre> </li> <li> <p>Run your task. The console output will include the URL of the task's RESULTS page.</p> <pre><code>TRAINS Task: overwriting (reusing) task id=60763e04c0ba45ea9fe3cfe79f3f06a3\nTRAINS results page: https://demoapp.trains.allegro.ai/projects/21643e0f1c4a4c99953302fc88a1a84c/experiments/60763e04c0ba45ea9fe3cfe79f3f06a3/output/log&lt;/code&gt;&lt;/pre&gt;\n</code></pre> </li> </ol> <p>See an example script here.</p>"},{"location":"extensions/trains/#tracking-your-autokeras-tasks","title":"Tracking your AutoKeras tasks","text":""},{"location":"extensions/trains/#visualizing-task-results","title":"Visualizing Task Results","text":"<p>Trains automatically logs comprehensive information about your AutoKeras task: code source control, execution environment, hyperparameters and more. It also automatically records any scalars, histograms and images reported to Tensorboard/Matplotlib or Seaborn.</p> <p>For example, making use of Tensorboard in your task will make all recorded information available in Trains as well:</p> <pre><code>from tensorflow import keras\n\ntensorboard_callback_train = keras.callbacks.TensorBoard(log_dir='log')\ntensorboard_callback_test = keras.callbacks.TensorBoard(log_dir='log')\nclf.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback_train])\nclf.fit(x_test, y_test, epochs=2, callbacks=[tensorboard_callback_test])\n</code></pre> <p>When your task runs, you can follow its results, including any collected metrics through the Trains web UI.</p> <p>View your task results in the Trains web UI, by clicking on it in the EXPERIMENTS table. Find the EXPERIMENT table under the specified project listed in the HOME or PROJECTS page:  </p> <p></p> <p>Detailed description Trains Web UI experiment information can be obtained here. Additional information on Trains logging capabilities can be obtained in the relevant Trains Documentation</p>"},{"location":"extensions/trains/#task-models","title":"Task Models","text":"<p>Trains automatically tracks models produced by your AutoKeras tasks.</p> <p>To upload models, specify the <code>output_uri</code> parameter when calling <code>Task.init</code> to provide the upload destination:</p> <pre><code>    task = Task.init(project_name=\"autokeras\", \n        task_name=\"autokeras imdb example with scalars\",\n        output_uri=\"http://localhost:8081/\")\n</code></pre> <p>View models information in the experiment details panel, ARTIFACTS tab:  </p> <p></p>"},{"location":"extensions/trains/#tracking-model-performance","title":"Tracking Model Performance","text":"<p>Use the Trains web UI to easily create experiment leaderboards and quickly identify best performing models. Customize your board adding any valuable metric or hyperparameter.</p> <p> </p> <p>Additional information on customizing Trains experiment and model tables can be obtained in the relevant Trains Documentation</p>"},{"location":"extensions/trains/#model-development-insights","title":"Model Development Insights","text":"<p>Use the Trains web UI to view side-by-side comparison of experiments: Easily locate the differences and impact of experiment configuration parameters, metrics, scalars etc.</p> <p>Compare multiple experiments, by selecting two or more experiments in the EXPERIMENTS table, and clicking COMPARE.</p> <p>The following image shows how two experiments compare in their epoch_accuracy and epoch_loss behaviour:</p> <p></p>"},{"location":"tutorial/customized/","title":"Customized Model","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import keras\nimport numpy as np\nimport tree\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre> <p>In this tutorial, we show how to customize your search space with AutoModel and how to implement your own block as search space.  This API is mainly for advanced users who already know what their model should look like.</p>"},{"location":"tutorial/customized/#customized-search-space","title":"Customized Search Space","text":"<p>First, let us see how we can build the following neural network using the building blocks in AutoKeras.</p>  graph LR     id1(ImageInput) --&gt; id2(Normalization)     id2 --&gt; id3(Image Augmentation)     id3 --&gt; id4(Convolutional)     id3 --&gt; id5(ResNet V2)     id4 --&gt; id6(Merge)     id5 --&gt; id6     id6 --&gt; id7(Classification Head)  <p>We can make use of the AutoModel API in AutoKeras to implemented as follows. The usage is the same as the Keras functional API. Since this is just a demo, we use small amount of <code>max_trials</code> and <code>epochs</code>.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node = ak.ClassificationHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\n</code></pre> <p>Whild building the model, the blocks used need to follow this topology: <code>Preprocessor</code> -&gt; <code>Block</code> -&gt; <code>Head</code>. <code>Normalization</code> and <code>ImageAugmentation</code> are <code>Preprocessor</code>s. <code>ClassificationHead</code> is <code>Head</code>. The rest are <code>Block</code>s.</p> <p>In the code above, we use <code>ak.ResNetBlock(version='v2')</code> to specify the version of ResNet to use.  There are many other arguments to specify for each building block.  For most of the arguments, if not specified, they would be tuned automatically.  Please refer to the documentation links at the bottom of the page for more details.</p> <p>Then, we prepare some data to run the model.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n\n# Feed the AutoModel with training data.\nauto_model.fit(x_train[:100], y_train[:100], epochs=1)\n# Predict with the best model.\npredicted_y = auto_model.predict(x_test)\n# Evaluate the best model with testing data.\nprint(auto_model.evaluate(x_test, y_test))\n</code></pre> <p>For multiple input nodes and multiple heads search space, you can refer to this section.</p>"},{"location":"tutorial/customized/#validation-data","title":"Validation Data","text":"<p>If you would like to provide your own validation data or change the ratio of the validation data, please refer to the Validation Data section of the tutorials of Image Classification, Text Classification, Multi-task and Multiple Validation.</p>"},{"location":"tutorial/customized/#data-format","title":"Data Format","text":"<p>You can refer to the documentation of ImageInput, TextInput, RegressionHead, ClassificationHead, for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification, Text Classification.</p>"},{"location":"tutorial/customized/#implement-new-block","title":"Implement New Block","text":"<p>You can extend the Block class to implement your own building blocks and use it with AutoModel.</p> <p>The first step is to learn how to write a build function for KerasTuner.  You need to override the build function of the block.  The following example shows how to implement a single Dense layer block whose number of neurons is tunable.</p> <pre><code>class SingleDenseLayerBlock(ak.Block):\n    def build(self, hp, inputs=None):\n        # Get the input_node from inputs.\n        input_node = tree.flatten(inputs)[0]\n        layer = keras.layers.Dense(\n            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n        )\n        output_node = layer(input_node)\n        return output_node\n</code></pre> <p>You can connect it with other blocks and build it into an AutoModel.</p> <pre><code># Build the AutoModel\ninput_node = ak.Input()\noutput_node = SingleDenseLayerBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nauto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=1)\n# Prepare Data\nnum_instances = 100\nx_train = np.random.rand(num_instances, 20).astype(np.float32)\ny_train = np.random.rand(num_instances, 1).astype(np.float32)\nx_test = np.random.rand(num_instances, 20).astype(np.float32)\ny_test = np.random.rand(num_instances, 1).astype(np.float32)\n# Train the model\nauto_model.fit(x_train, y_train, epochs=1)\nprint(auto_model.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/customized/#reference","title":"Reference","text":"<p>AutoModel</p> <p>Nodes: ImageInput, Input, TextInput.</p> <p>Preprocessors: FeatureEngineering, ImageAugmentation, LightGBM, Normalization,</p> <p>Blocks: ConvBlock, DenseBlock, Merge, ResNetBlock, RNNBlock, SpatialReduction, TemporalReduction, XceptionBlock, ImageBlock, TextBlock.</p>"},{"location":"tutorial/export/","title":"Export Model","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import load_model\n\nimport autokeras as ak\n</code></pre> <p>You can easily export your model the best model found by AutoKeras as a Keras Model.</p> <p>The following example uses ImageClassifier as an example. All the tasks and the AutoModel has this export_model function.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Initialize the image classifier.\nclf = ak.ImageClassifier(\n    overwrite=True, max_trials=1\n)  # Try only 1 model.(Increase accordingly)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=1)  # Change no of epochs to improve the model\n# Export as a Keras Model.\nmodel = clf.export_model()\n\nprint(type(model))  # &lt;class 'tensorflow.python.keras.engine.training.Model'&gt;\n\nmodel.save(\"model_autokeras.keras\")\n\n\nloaded_model = load_model(\n    \"model_autokeras.keras\", custom_objects=ak.CUSTOM_OBJECTS\n)\n\npredicted_y = loaded_model.predict(np.expand_dims(x_test, -1))\nprint(predicted_y)\n</code></pre>"},{"location":"tutorial/faq/","title":"FAQ","text":""},{"location":"tutorial/faq/#how-to-resume-a-previously-killed-run","title":"How to resume a previously killed run?","text":"<p>This feature is controlled by the <code>overwrite</code> argument of <code>AutoModel</code> or any other task APIs. It is set to <code>False</code> by default, which means it would not overwrite the contents of the directory. In other words, it will continue the previous fit.</p> <p>You can just run the same code again. It will automatically resume the previously killed run.</p>"},{"location":"tutorial/faq/#how-to-customize-metrics-and-loss","title":"How to customize metrics and loss?","text":"<p>Please see the code example below.</p> <pre><code>import autokeras as ak\n\n\nclf = ak.ImageClassifier(\n    max_trials=3,\n    metrics=['mse'],\n    loss='mse',\n)\n</code></pre>"},{"location":"tutorial/faq/#how-to-use-customized-metrics-to-select-the-best-model","title":"How to use customized metrics to select the best model?","text":"<p>By default, AutoKeras use validation loss as the metric for selecting the best model. Below is a code example of using customized metric for selecting models. Please read the comments for the details.</p> <pre><code># Implement your customized metric according to the tutorial.\n# https://keras.io/api/metrics/#creating-custom-metrics\nimport autokeras as ak\n\n\ndef f1_score(y_true, y_pred):\n  ...\n\nclf = ak.ImageClassifier(\n    max_trials=3,\n\n    # Wrap the function into a Keras Tuner Objective \n    # and pass it to AutoKeras.\n\n    # Direction can be 'min' or 'max'\n    # meaning we want to minimize or maximize the metric.\n\n    # 'val_f1_score' is just add a 'val_' prefix\n    # to the function name or the metric name.\n\n    objective=kerastuner.Objective('val_f1_score', direction='max'),\n    # Include it as one of the metrics.\n    metrics=[f1_score],\n)\n</code></pre>"},{"location":"tutorial/faq/#how-to-use-multiple-gpus","title":"How to use multiple GPUs?","text":"<p>You can use the <code>distribution_strategy</code> argument when initializing any model you created with AutoKeras, like AutoModel, ImageClassifier and so on. This argument is supported by Keras Tuner. AutoKeras supports the arguments supported by Keras Tuner. Please see the discription of the argument here.</p> <pre><code>import tensorflow as tf\nimport autokeras as ak\n\n\nauto_model = ak.ImageClassifier(\n    max_trials=3,\n    distribution_strategy=tf.distribute.MirroredStrategy(),\n)\n</code></pre>"},{"location":"tutorial/faq/#how-to-constrain-the-model-size","title":"How to constrain the model size?","text":"<p>You can use the <code>max_model_size</code> argument for any model in AutoKeras.</p> <pre><code>import autokeras as ak\n\n\nauto_model = ak.ImageClassifier(\n    max_trials=3,\n    max_model_size=1000000000,\n)\n</code></pre>"},{"location":"tutorial/image_classification/","title":"Image Classification","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import numpy as np\nimport tensorflow as tf\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/image_classification/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the MNIST dataset as an example</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:100]\ny_train = y_train[:100]\nx_test = x_test[:100]\ny_test = y_test[:100]\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n</code></pre> <p>The second step is to run the ImageClassifier. It is recommended have more trials for more complicated datasets. This is just a quick demo of MNIST, so we set max_trials to 1. For the same reason, we set epochs to 10. You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the image classifier.\nclf = ak.ImageClassifier(overwrite=True, max_trials=1)\n# Feed the image classifier with training data.\nclf.fit(x_train, y_train, epochs=1)\n\n\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\nprint(predicted_y)\n\n\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/image_classification/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.</p> <pre><code>clf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with validation_data.</p> <pre><code>split = 50000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=1,\n)\n</code></pre>"},{"location":"tutorial/image_classification/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of ImageClassifier. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=True,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1)\n</code></pre> <p>The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.some_block(input_node).</p> <p>You can even also use more fine grained blocks to customize the search space even further. See the following example.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1)\n</code></pre>"},{"location":"tutorial/image_classification/#data-format","title":"Data Format","text":"<p>The AutoKeras ImageClassifier is quite flexible for the data format.</p> <p>For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1).</p> <p>For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s.</p> <p>So if you prepare your data in the following way, the ImageClassifier should still work.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Reshape the images to have the channel dimension.\nx_train = x_train.reshape(x_train.shape + (1,))\nx_test = x_test.reshape(x_test.shape + (1,))\n\n# One-hot encode the labels.\neye = np.eye(10)\ny_train = eye[y_train]\ny_test = eye[y_test]\n\nprint(x_train.shape)  # (60000, 28, 28, 1)\nprint(y_train.shape)  # (60000, 10)\nprint(y_train[:3])\n# array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n#        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n</code></pre> <p>We also support using tf.data.Dataset format for the training data.</p> <pre><code>train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n\nclf = ak.ImageClassifier(overwrite=True, max_trials=1)\n# Feed the tensorflow Dataset to the classifier.\nclf.fit(train_set, epochs=1)\n# Predict with the best model.\npredicted_y = clf.predict(test_set)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set))\n</code></pre>"},{"location":"tutorial/image_classification/#reference","title":"Reference","text":"<p>ImageClassifier, AutoModel, ImageBlock, Normalization, ImageAugmentation, ResNetBlock, ImageInput, ClassificationHead.</p>"},{"location":"tutorial/image_regression/","title":"Image Regression","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import tensorflow as tf\nfrom keras.datasets import mnist\n\nimport autokeras as ak\n</code></pre> <p>To make this tutorial easy to follow, we just treat MNIST dataset as a regression dataset. It means we will treat prediction targets of MNIST dataset, which are integers ranging from 0 to 9 as numerical values, so that they can be directly used as the regression targets.</p>"},{"location":"tutorial/image_regression/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the MNIST dataset as an example</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:100]\ny_train = y_train[:100]\nx_test = x_test[:100]\ny_test = y_test[:100]\nprint(x_train.shape)  # (60000, 28, 28)\nprint(y_train.shape)  # (60000,)\nprint(y_train[:3])  # array([7, 2, 1], dtype=uint8)\n</code></pre> <p>The second step is to run the ImageRegressor.  It is recommended have more trials for more complicated datasets.  This is just a quick demo of MNIST, so we set max_trials to 1.  For the same reason, we set epochs to 1.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the image regressor.\nreg = ak.ImageRegressor(overwrite=True, max_trials=1)\n# Feed the image regressor with training data.\nreg.fit(x_train, y_train, epochs=1)\n\n\n# Predict with the best model.\npredicted_y = reg.predict(x_test)\nprint(predicted_y)\n\n\n# Evaluate the best model with testing data.\nprint(reg.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/image_regression/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.</p> <pre><code>reg.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with validation_data.</p> <pre><code>split = 50000\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nreg.fit(\n    x_train,\n    y_train,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    epochs=2,\n)\n</code></pre>"},{"location":"tutorial/image_regression/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of ImageRegressor. You can configure the ImageBlock for some high-level configurations, e.g., block_type for the type of neural network to search, normalize for whether to do data normalization, augment for whether to do data augmentation. You can also do not specify these arguments, which would leave the different choices to be tuned automatically. See the following example for detail.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.ImageBlock(\n    # Only search ResNet architectures.\n    block_type=\"resnet\",\n    # Normalize the dataset.\n    normalize=False,\n    # Do not do data augmentation.\n    augment=False,\n)(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1)\n</code></pre> <p>The usage of AutoModel is similar to the functional API of Keras. Basically, you are building a graph, whose edges are blocks and the nodes are intermediate outputs of blocks. To add an edge from input_node to output_node with output_node = ak.some_block(input_node).</p> <p>You can even also use more fine grained blocks to customize the search space even further. See the following example.</p> <pre><code>input_node = ak.ImageInput()\noutput_node = ak.Normalization()(input_node)\noutput_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\noutput_node = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1)\n</code></pre>"},{"location":"tutorial/image_regression/#data-format","title":"Data Format","text":"<p>The AutoKeras ImageRegressor is quite flexible for the data format.</p> <p>For the image, it accepts data formats both with and without the channel dimension. The images in the MNIST dataset do not have the channel dimension. Each image is a matrix with shape (28, 28). AutoKeras also accepts images of three dimensions with the channel dimension at last, e.g., (32, 32, 3), (28, 28, 1).</p> <p>For the regression targets, it should be a vector of numerical values. AutoKeras accepts numpy.ndarray.</p> <p>We also support using tf.data.Dataset format for the training data. In this case, the images would have to be 3-dimentional.</p> <pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:100]\ny_train = y_train[:100]\nx_test = x_test[:100]\ny_test = y_test[:100]\n\n# Reshape the images to have the channel dimension.\nx_train = x_train.reshape(x_train.shape + (1,))\nx_test = x_test.reshape(x_test.shape + (1,))\ny_train = y_train.reshape(y_train.shape + (1,))\ny_test = y_test.reshape(y_test.shape + (1,))\n\nprint(x_train.shape)  # (60000, 28, 28, 1)\nprint(y_train.shape)  # (60000, 10)\n\ntrain_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n\nreg = ak.ImageRegressor(overwrite=True, max_trials=1)\n# Feed the tensorflow Dataset to the regressor.\nreg.fit(train_set, epochs=1)\n# Predict with the best model.\npredicted_y = reg.predict(test_set)\n# Evaluate the best model with testing data.\nprint(reg.evaluate(test_set))\n</code></pre>"},{"location":"tutorial/image_regression/#reference","title":"Reference","text":"<p>ImageRegressor, AutoModel, ImageBlock, Normalization, ImageAugmentation, ResNetBlock, ImageInput, RegressionHead.</p>"},{"location":"tutorial/load/","title":"Load Data from Disk","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import os\nimport shutil\n\nimport keras\nimport numpy as np\nimport tensorflow as tf\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/load/#load-images-from-disk","title":"Load Images from Disk","text":"<p>If the data is too large to put in memory all at once, we can load it batch by batch into memory from disk with tf.data.Dataset.  This function can help you build such a tf.data.Dataset for image data.</p> <p>First, we download the data and extract the files.</p> <pre><code>dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"  # noqa: E501\nlocal_file_path = keras.utils.get_file(\n    origin=dataset_url, fname=\"image_data\", extract=True\n)\n# The file is extracted in the same directory as the downloaded file.\nlocal_dir_path = os.path.dirname(local_file_path)\n# After check mannually, we know the extracted data is in 'flower_photos'.\ndata_dir = os.path.join(local_dir_path, \"flower_photos\")\nprint(data_dir)\n</code></pre> <p>The directory should look like this. Each folder contains the images in the same class.</p> <pre><code>flowers_photos/\n  daisy/\n  dandelion/\n  roses/\n  sunflowers/\n  tulips/\n</code></pre> <p>We can split the data into training and testing as we load them.</p> <pre><code>batch_size = 2\nimg_height = 180\nimg_width = 180\n\ntrain_data = ak.image_dataset_from_directory(\n    data_dir,\n    # Use 20% data as testing data.\n    validation_split=0.2,\n    subset=\"training\",\n    # Set seed to ensure the same split when loading testing data.\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n)\n\ntest_data = ak.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size,\n)\n</code></pre> <p>Then we just do one quick demo of AutoKeras to make sure the dataset works.</p> <pre><code>clf = ak.ImageClassifier(overwrite=True, max_trials=1)\nclf.fit(train_data.take(100), epochs=1)\nprint(clf.evaluate(test_data.take(2)))\n</code></pre>"},{"location":"tutorial/load/#load-texts-from-disk","title":"Load Texts from Disk","text":"<p>You can also load text datasets in the same way.</p> <pre><code>dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n\nlocal_file_path = keras.utils.get_file(\n    fname=\"text_data\",\n    origin=dataset_url,\n    extract=True,\n)\n# The file is extracted in the same directory as the downloaded file.\nlocal_dir_path = os.path.dirname(local_file_path)\n# After check mannually, we know the extracted data is in 'aclImdb'.\ndata_dir = os.path.join(local_dir_path, \"aclImdb\")\n# Remove the unused data folder.\n\nshutil.rmtree(os.path.join(data_dir, \"train/unsup\"))\n</code></pre> <p>For this dataset, the data is already split into train and test. We just load them separately.</p> <pre><code>print(data_dir)\ntrain_data = ak.text_dataset_from_directory(\n    os.path.join(data_dir, \"train\"), batch_size=batch_size\n)\n\ntest_data = ak.text_dataset_from_directory(\n    os.path.join(data_dir, \"test\"), shuffle=False, batch_size=batch_size\n)\n\nclf = ak.TextClassifier(overwrite=True, max_trials=1)\nclf.fit(train_data.take(2), epochs=1)\nprint(clf.evaluate(test_data.take(2)))\n</code></pre>"},{"location":"tutorial/load/#load-data-with-python-generators","title":"Load Data with Python Generators","text":"<p>If you want to use generators, you can refer to the following code.</p> <pre><code>N_BATCHES = 2\nBATCH_SIZE = 10\n\n\ndef get_data_generator(n_batches, batch_size):\n    \"\"\"Get a generator returning n_batches random data.\"\"\"\n\n    def data_generator():\n        for _ in range(n_batches * batch_size):\n            x = np.random.randn(32, 32, 3)\n            y = x.sum() / 32 * 32 * 3 &gt; 0.5\n            yield x, y\n\n    return data_generator\n\n\ndataset = tf.data.Dataset.from_generator(\n    get_data_generator(N_BATCHES, BATCH_SIZE),\n    output_types=(tf.float32, tf.float32),\n    output_shapes=((32, 32, 3), tuple()),\n).batch(BATCH_SIZE)\n\nclf = ak.ImageClassifier(overwrite=True, max_trials=1, seed=5)\nclf.fit(x=dataset, validation_data=dataset, batch_size=BATCH_SIZE)\nprint(clf.evaluate(dataset))\n</code></pre>"},{"location":"tutorial/load/#reference","title":"Reference","text":"<p>image_dataset_from_directory text_dataset_from_directory</p>"},{"location":"tutorial/multi/","title":"Multi-Modal and Multi-Task","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import numpy as np\n\nimport autokeras as ak\n</code></pre> <p>In this tutorial we are making use of the AutoModel  API to show how to handle multi-modal data and multi-task.</p>"},{"location":"tutorial/multi/#what-is-multi-modal","title":"What is multi-modal?","text":"<p>Multi-modal data means each data instance has multiple forms of information. For example, a photo can be saved as a image. Besides the image, it may also have when and where it was taken as its attributes, which can be represented as numerical data.</p>"},{"location":"tutorial/multi/#what-is-multi-task","title":"What is multi-task?","text":"<p>Multi-task here we refer to we want to predict multiple targets with the same input features. For example, we not only want to classify an image according to its content, but we also want to regress its quality as a float number between 0 and 1.</p> <p>The following diagram shows an example of multi-modal and multi-task neural network model.</p>  graph TD     id1(ImageInput) --&gt; id3(Some Neural Network Model)     id2(Input) --&gt; id3     id3 --&gt; id4(ClassificationHead)     id3 --&gt; id5(RegressionHead)  <p>It has two inputs the images and the numerical input data. Each image is associated with a set of attributes in the numerical input data. From these data, we are trying to predict the classification label and the regression value at the same time.</p>"},{"location":"tutorial/multi/#data-preparation","title":"Data Preparation","text":"<p>To illustrate our idea, we generate some random image and numerical data as the multi-modal data.</p> <pre><code>num_instances = 10\n# Generate image data.\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\n# Generate numerical data.\nnumerical_data = np.random.rand(num_instances, 20).astype(np.float32)\n</code></pre> <p>We also generate some multi-task targets for classification and regression.</p> <pre><code># Generate regression targets.\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\n# Generate classification labels of five classes.\nclassification_target = np.random.randint(5, size=num_instances)\n</code></pre>"},{"location":"tutorial/multi/#build-and-train-the-model","title":"Build and Train the Model","text":"<p>Then we initialize the multi-modal and multi-task model with AutoModel. Since this is just a demo, we use small amount of <code>max_trials</code> and <code>epochs</code>.</p> <pre><code># Initialize the multi with multiple inputs and outputs.\nmodel = ak.AutoModel(\n    inputs=[ak.ImageInput(), ak.Input()],\n    outputs=[\n        ak.RegressionHead(metrics=[\"mae\"]),\n        ak.ClassificationHead(\n            loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n        ),\n    ],\n    overwrite=True,\n    max_trials=2,\n)\n# Fit the model with prepared data.\nmodel.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    epochs=1,\n    batch_size=3,\n)\n</code></pre>"},{"location":"tutorial/multi/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>model.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n    batch_size=3,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\n\nimage_val = image_data[split:]\nnumerical_val = numerical_data[split:]\nregression_val = regression_target[split:]\nclassification_val = classification_target[split:]\n\nimage_data = image_data[:split]\nnumerical_data = numerical_data[:split]\nregression_target = regression_target[:split]\nclassification_target = classification_target[:split]\n\nmodel.fit(\n    [image_data, numerical_data],\n    [regression_target, classification_target],\n    # Use your own validation set.\n    validation_data=(\n        [image_val, numerical_val],\n        [regression_val, classification_val],\n    ),\n    epochs=1,\n    batch_size=3,\n)\n</code></pre>"},{"location":"tutorial/multi/#customized-search-space","title":"Customized Search Space","text":"<p>You can customize your search space. The following figure shows the search space we want to define.</p>  graph LR     id1(ImageInput) --&gt; id2(Normalization)     id2 --&gt; id3(Image Augmentation)     id3 --&gt; id4(Convolutional)     id3 --&gt; id5(ResNet V2)     id4 --&gt; id6(Merge)     id5 --&gt; id6     id7(Input) --&gt; id9(DenseBlock)     id6 --&gt; id10(Merge)     id9 --&gt; id10     id10 --&gt; id11(Classification Head)     id10 --&gt; id12(Regression Head)  <pre><code>input_node1 = ak.ImageInput()\noutput_node = ak.Normalization()(input_node1)\noutput_node = ak.ImageAugmentation()(output_node)\noutput_node1 = ak.ConvBlock()(output_node)\noutput_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\noutput_node1 = ak.Merge()([output_node1, output_node2])\n\ninput_node2 = ak.Input()\noutput_node2 = ak.DenseBlock()(input_node2)\n\noutput_node = ak.Merge()([output_node1, output_node2])\noutput_node1 = ak.ClassificationHead()(output_node)\noutput_node2 = ak.RegressionHead()(output_node)\n\nauto_model = ak.AutoModel(\n    inputs=[input_node1, input_node2],\n    outputs=[output_node1, output_node2],\n    overwrite=True,\n    max_trials=2,\n)\n\nimage_data = np.random.rand(num_instances, 32, 32, 3).astype(np.float32)\nnumerical_data = np.random.rand(num_instances, 20).astype(np.float32)\nregression_target = np.random.rand(num_instances, 1).astype(np.float32)\nclassification_target = np.random.randint(5, size=num_instances)\n\nauto_model.fit(\n    [image_data, numerical_data],\n    [classification_target, regression_target],\n    batch_size=3,\n    epochs=1,\n)\n</code></pre>"},{"location":"tutorial/multi/#data-format","title":"Data Format","text":"<p>You can refer to the documentation of ImageInput, Input, TextInput, RegressionHead, ClassificationHead, for the format of different types of data. You can also refer to the Data Format section of the tutorials of Image Classification, Text Classification,</p>"},{"location":"tutorial/multi/#reference","title":"Reference","text":"<p>AutoModel, ImageInput, Input, DenseBlock, RegressionHead, ClassificationHead, CategoricalToNumerical.</p>"},{"location":"tutorial/overview/","title":"AutoKeras 1.0 Tutorial","text":""},{"location":"tutorial/overview/#supported-tasks","title":"Supported Tasks","text":"<p>AutoKeras supports several tasks with an extremely simple interface. You can click the links below to see the detailed tutorial for each task.</p> <p>Supported Tasks:</p> <p>Image Classification</p> <p>Image Regression</p> <p>Text Classification</p> <p>Text Regression</p>"},{"location":"tutorial/overview/#multi-task-and-multi-modal-data","title":"Multi-Task and Multi-Modal Data","text":"<p>If you are dealing with multi-task or multi-modal dataset, you can refer to this tutorial for details.</p>"},{"location":"tutorial/overview/#customized-model","title":"Customized Model","text":"<p>Follow this tutorial, to use AutoKeras building blocks to quickly construct your own model. With these blocks, you only need to specify the high-level architecture of your model. AutoKeras would search for the best detailed configuration for you. Moreover, you can override the base classes to create your own block. The following are the links to the documentation of the predefined input nodes and blocks in AutoKeras.</p> <p>Nodes:</p> <p>ImageInput</p> <p>Input</p> <p>TextInput</p> <p>Blocks:</p> <p>ImageAugmentation</p> <p>Normalization</p> <p>ConvBlock</p> <p>DenseBlock</p> <p>Merge</p> <p>ResNetBlock</p> <p>RNNBlock</p> <p>SpatialReduction</p> <p>TemporalReduction</p> <p>XceptionBlock</p> <p>ImageBlock</p> <p>TextBlock</p> <p>ClassificationHead</p> <p>RegressionHead</p>"},{"location":"tutorial/overview/#export-model","title":"Export Model","text":"<p>You can follow this tutorial to export the best model.</p>"},{"location":"tutorial/text_classification/","title":"Text Classification","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import os\n\nimport keras\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.datasets import load_files\n\nimport autokeras as ak\n</code></pre>"},{"location":"tutorial/text_classification/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the IMDB dataset as an example.</p> <pre><code>dataset = keras.utils.get_file(\n    fname=\"aclImdb.tar.gz\",\n    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n    extract=True,\n)\n\n# set path to dataset\nIMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n\nclasses = [\"pos\", \"neg\"]\ntrain_data = load_files(\n    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n)\ntest_data = load_files(\n    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n)\n\nx_train = np.array(train_data.data)[:100]\ny_train = np.array(train_data.target)[:100]\nx_test = np.array(test_data.data)[:100]\ny_test = np.array(test_data.target)[:100]\n\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # this film was just brilliant casting\n</code></pre> <p>The second step is to run the TextClassifier.  As a quick demo, we set epochs to 2.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the text classifier.\nclf = ak.TextClassifier(\n    overwrite=True, max_trials=1\n)  # It only tries 1 model as a quick demo.\n# Feed the text classifier with training data.\nclf.fit(x_train, y_train, epochs=1, batch_size=2)\n# Predict with the best model.\npredicted_y = clf.predict(x_test)\n# Evaluate the best model with testing data.\nprint(clf.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/text_classification/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>clf.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n    epochs=1,\n    batch_size=2,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nclf.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    batch_size=2,\n)\n</code></pre>"},{"location":"tutorial/text_classification/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of TextClassifier. You can configure the TextBlock for some high-level configurations. You can also do not specify these arguments, which would leave the different choices to be tuned automatically.  See the following example for detail.</p> <pre><code>input_node = ak.TextInput()\noutput_node = ak.TextBlock()(input_node)\noutput_node = ak.ClassificationHead()(output_node)\nclf = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nclf.fit(x_train, y_train, epochs=1, batch_size=2)\n</code></pre>"},{"location":"tutorial/text_classification/#data-format","title":"Data Format","text":"<p>The AutoKeras TextClassifier is quite flexible for the data format.</p> <p>For the text, the input data should be one-dimensional For the classification labels, AutoKeras accepts both plain labels, i.e. strings or integers, and one-hot encoded encoded labels, i.e. vectors of 0s and 1s.</p> <p>We also support using tf.data.Dataset format for the training data.</p> <pre><code>train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(\n    2\n)\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)\n\nclf = ak.TextClassifier(overwrite=True, max_trials=1)\n# Feed the tensorflow Dataset to the classifier.\nclf.fit(train_set.take(2), epochs=1)\n# Predict with the best model.\npredicted_y = clf.predict(test_set.take(2))\n# Evaluate the best model with testing data.\nprint(clf.evaluate(test_set.take(2)))\n</code></pre>"},{"location":"tutorial/text_classification/#reference","title":"Reference","text":"<p>TextClassifier, AutoModel, ConvBlock, TextInput, ClassificationHead.</p>"},{"location":"tutorial/text_regression/","title":"Text Regression","text":"<p> View in Colab GitHub source</p> <pre><code>!pip install autokeras\n</code></pre> <pre><code>import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.datasets import load_files\n\nimport autokeras as ak\n</code></pre> <p>To make this tutorial easy to follow, we just treat IMDB dataset as a regression dataset. It means we will treat prediction targets of IMDB dataset, which are 0s and 1s as numerical values, so that they can be directly used as the regression targets.</p>"},{"location":"tutorial/text_regression/#a-simple-example","title":"A Simple Example","text":"<p>The first step is to prepare your data. Here we use the IMDB dataset as an example.</p> <pre><code>dataset = tf.keras.utils.get_file(\n    fname=\"aclImdb.tar.gz\",\n    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n    extract=True,\n)\n\n# set path to dataset\nIMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n\nclasses = [\"pos\", \"neg\"]\ntrain_data = load_files(\n    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n)\ntest_data = load_files(\n    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n)\n\nx_train = np.array(train_data.data)[:100]\ny_train = np.array(train_data.target)[:100]\nx_test = np.array(test_data.data)[:100]\ny_test = np.array(test_data.target)[:100]\n\nprint(x_train.shape)  # (25000,)\nprint(y_train.shape)  # (25000, 1)\nprint(x_train[0][:50])  # &lt;START&gt; this film was just brilliant casting &lt;UNK&gt;\n</code></pre> <p>The second step is to run the TextRegressor.  As a quick demo, we set epochs to 2.  You can also leave the epochs unspecified for an adaptive number of epochs.</p> <pre><code># Initialize the text regressor.\nreg = ak.TextRegressor(\n    overwrite=True, max_trials=1  # It tries 10 different models.\n)\n# Feed the text regressor with training data.\nreg.fit(x_train, y_train, epochs=1, batch_size=2)\n# Predict with the best model.\npredicted_y = reg.predict(x_test)\n# Evaluate the best model with testing data.\nprint(reg.evaluate(x_test, y_test))\n</code></pre>"},{"location":"tutorial/text_regression/#validation-data","title":"Validation Data","text":"<p>By default, AutoKeras use the last 20% of training data as validation data.  As shown in the example below, you can use <code>validation_split</code> to specify the percentage.</p> <pre><code>reg.fit(\n    x_train,\n    y_train,\n    # Split the training data and use the last 15% as validation data.\n    validation_split=0.15,\n)\n</code></pre> <p>You can also use your own validation set instead of splitting it from the training data with <code>validation_data</code>.</p> <pre><code>split = 5\nx_val = x_train[split:]\ny_val = y_train[split:]\nx_train = x_train[:split]\ny_train = y_train[:split]\nreg.fit(\n    x_train,\n    y_train,\n    epochs=1,\n    # Use your own validation set.\n    validation_data=(x_val, y_val),\n    batch_size=2,\n)\n</code></pre>"},{"location":"tutorial/text_regression/#customized-search-space","title":"Customized Search Space","text":"<p>For advanced users, you may customize your search space by using AutoModel instead of TextRegressor. You can configure the TextBlock for some high-level configurations. You can also do not specify these arguments, which would leave the different choices to be tuned automatically.  See the following example for detail.</p> <pre><code>input_node = ak.TextInput()\noutput_node = ak.TextBlock()(input_node)\noutput_node = ak.RegressionHead()(output_node)\nreg = ak.AutoModel(\n    inputs=input_node, outputs=output_node, overwrite=True, max_trials=1\n)\nreg.fit(x_train, y_train, epochs=1, batch_size=2)\n</code></pre>"},{"location":"tutorial/text_regression/#data-format","title":"Data Format","text":"<p>The AutoKeras TextRegressor is quite flexible for the data format.</p> <p>For the text, the input data should be one-dimensional For the regression targets, it should be a vector of numerical values.  AutoKeras accepts numpy.ndarray.</p> <p>We also support using tf.data.Dataset format for the training data.</p> <pre><code>train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,))).batch(\n    2\n)\ntest_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,))).batch(2)\n\nreg = ak.TextRegressor(overwrite=True, max_trials=2)\n# Feed the tensorflow Dataset to the regressor.\nreg.fit(train_set.take(2), epochs=1)\n# Predict with the best model.\npredicted_y = reg.predict(test_set.take(2))\n# Evaluate the best model with testing data.\nprint(reg.evaluate(test_set.take(2)))\n</code></pre>"},{"location":"tutorial/text_regression/#reference","title":"Reference","text":"<p>TextRegressor, AutoModel, TextBlock, ConvBlock, TextInput, RegressionHead.</p>"}]}